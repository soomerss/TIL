# 크롤링

## 크롤링이란 ? 
* 웹 사이트로 부터 데이터를 가져 오는 것을 말한다.
* 크롤링 기법에는 bs4의 BeautifulSoup을 사용한 방법, Selenium, Scrapy를 이용한 방법이 있다.
* 번외로 크롤링은 매번 바뀌는 웹사이트에서 정보를 얻어내야 하는 한계가 있지만, OPEN API를 활용하게 된다면 JSON 또는 XML등으로 규격화된 정보를 손쉽게 받아볼 수 있다. 다만, 모든 웹사이트가 정보를 제공하지 않을 뿐만 아니라 제공되지 않는 정보가 있을 수 있어 크롤링 기법을 함께 사용한다면 더욱 다양한 데이터를 얻을 수 있다.

## 비교
* bs4를 사용한다면 정적인 웹페이지를 가져와 파싱하고 여기서 원하는 정보를 찾아갈 수 있다. 또한 OPEN API를 통해 얻은 데이터를 파싱하기에도 용이하다. 다만 동적인 웹페이지를 얻을 수 없을 뿐만 아니라 크롤링 하려고 하는 페이지가 많을 경우 시간상 한계가 있다. 따라서 동적인 웹페이지를 하고 싶은 경우 Selenium, 대량의 데이터를 크롤링 하고 싶은 경우 Scrapy를 사용해야 한다.
* 웹 스크래핑은 정보의 추출 관점에서 본다면 웹 크롤링은 웹페이지 자체 인덱싱(색인)
* 웹 스크래핑 : 날씨 데이터 가져오기, 주식 데이터 가져오기
* 웹 크롤링 : 검색 엔진의 웹 크롤러

## 기본 구조
* bs4의 BeautifulSoup을 활용하면 다음과 같다.
```python
from bs4 import BeatifulSoup
import requests

url = "https://www.naver.com"
header_param = {} # Open_API 사용시 필요할 것이다.
res = requsests.get(url)
soup = BeautifulSoup(res.content,'html.parser')
data = soup.find('h1') # h1 태그를 가진 첫번째 정보를 찾아줘
# 만약 파싱한 데이터 Soup이 json일 경우 데이터는 다음과 같이 얻을 수 있다.
json_data = data.json()
```
## 고려사항
- 웹스크래핑으로 어떤 목적을 달성하고자하는가?
- 서버에 영향을 많은 영향을 미치지는 않는지?

## 로봇 배제 프로토콜(REP)
- Robot Exclusion Protocol
- 이때 User-agent란 요청을 보내는 주체를 나타내는 '프로그램'을 말한다.
- 모든 User-agent에 대해서 url 접근을 거부
```
User-agent : *
Disallow : /
```
- 모든 User-agent에 대해서 url 접근을 허용
```
User-agent : *
allow : /
```

## /robots.txt
* URL 뒤에 위 문구를 붙이게 되면 로봇 배제 프로토콜에 대한 정보를 얻을 수 있다.